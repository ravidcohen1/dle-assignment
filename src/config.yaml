input_len: 1024
vocab_size: 512
embedding_dim: 256
attention_hidden_dim: 128
num_transformer_layers: 3
num_heads: 2
mlp_hidden_dim: 1280
norm_layer_eps: 1e-5

epsilon: 1e-5

scale_attention: true
gelu_approximation: none
transformer_forward_alternative: none
